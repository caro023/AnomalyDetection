{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602f4eac",
   "metadata": {},
   "source": [
    "Network Traffic Anomaly Detection using Deep Learning\n",
    "\n",
    "Dataset: UNSW-NB15 (Parquet format)\n",
    "Objective: Automatic identification of network attacks using Autoencoder-based approach\n",
    "\n",
    "This project demonstrates AI/ML techniques for cybersecurity, specifically:\n",
    "Unsupervised anomaly detection\n",
    "- Deep learning architectures\n",
    "- Network traffic analysis\n",
    "- Feature engineering for cybersecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bde54",
   "metadata": {},
   "source": [
    "Data loading and explorarion\n",
    "Load UNSW-NB15 dataset from parquet files.\n",
    "    \n",
    "    The dataset can be dowloaded as follows:\n",
    "    dataset_name = \"UNSW-NB15\"\n",
    "    subset = ['Network-Flows', 'Packet-Fields', 'Payload-Bytes']   # come nell'esempio, oppure 'all'\n",
    "    files = [3, 5, 10] \n",
    "    from nids_datasets import Dataset, DatasetInfo\n",
    "    data = Dataset(dataset=dataset_name, subset=subset, files=files)\n",
    "\n",
    "    data.download() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cd2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unsw_nb15(data_dir=\"UNSW-NB15\"):\n",
    "        \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Primary dataset file\n",
    "    parquet_file = data_path / \"Network-Flows\" / \"UNSW_Flow.parquet\"\n",
    "    \n",
    "    try:\n",
    "        if parquet_file.exists():\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            print(f\"Dataset loaded successfully: {df.shape[0]:,} samples, {df.shape[1]} features\\n\")\n",
    "            return df\n",
    "        else:\n",
    "            dataset_name = \"UNSW-NB15\"\n",
    "            subset = ['Network-Flows', 'Packet-Fields', 'Payload-Bytes']   \n",
    "            files = [3, 5, 10] \n",
    "            from nids_datasets import Dataset, DatasetInfo\n",
    "            data = Dataset(dataset=dataset_name, subset=subset, files=files)\n",
    "            data.download() \n",
    "            return \"Error in the dataset loading\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return (f\"Error loading dataset: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8146e",
   "metadata": {},
   "source": [
    "Data preprocessing and feature engineerin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4025fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataPreprocessor:\n",
    "    \"\"\"preprocessing of network traffic data\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit preprocessors and transform data\"\"\"\n",
    "        print(\"Preprocessing data...\\n\")\n",
    "        \n",
    "        # Check if binary_label already exists\n",
    "        if 'binary_label' in df.columns:\n",
    "            df['label'] = df['binary_label'].astype(int)\n",
    "            label_source = 'binary_label'\n",
    "        else:\n",
    "            raise ValueError(\"Could not find label column. Available columns: \" + str(list(df.columns)))\n",
    "        \n",
    "        #Show attack type distribution if available\n",
    "        if 'attack_label' in df.columns:\n",
    "            print(f\"\\n Attack type distribution:\")\n",
    "            attack_counts = df['attack_label'].value_counts()\n",
    "            for attack_type, count in attack_counts.head(10).items():\n",
    "                print(f\"   {attack_type}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n Label distribution:\")\n",
    "        print(f\"   Normal (0): {sum(df['label']==0):,} ({sum(df['label']==0)/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Attack (1): {sum(df['label']==1):,} ({sum(df['label']==1)/len(df)*100:.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        # The model in the training does not have the information if is an attack or not\n",
    "        exclude_cols = ['label', 'binary_label', 'attack_label']\n",
    "        \n",
    "        # Exclude network identifiers that identify a single connection/flow to avoid overfitting \n",
    "        exclude_patterns = ['id', 'ip', 'addr', 'port', 'time', 'src', 'dst', \n",
    "                           'source', 'dest', 'destination']\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(pattern in col_lower for pattern in exclude_patterns):\n",
    "                if col not in exclude_cols:\n",
    "                    exclude_cols.append(col)\n",
    "        \n",
    "        print(f\"Excluding {len(exclude_cols)} columns: {exclude_cols[:5]}...\")\n",
    "        print()\n",
    "        \n",
    "        # Separate categorical and numerical features\n",
    "        remaining_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "        \n",
    "        #string/objects columns\n",
    "        categorical_cols = []\n",
    "        #numerical columns\n",
    "        numerical_cols = []\n",
    "        \n",
    "        for col in remaining_cols:\n",
    "            dtype_str = str(df[col].dtype)\n",
    "            # Check if categorical\n",
    "            if dtype_str in ['object', 'string', 'category']:\n",
    "                categorical_cols.append(col)\n",
    "            elif df[col].nunique() < 20 and 'int' in dtype_str.lower():\n",
    "                # Low cardinality integers might be categorical (protocol flags)\n",
    "                categorical_cols.append(col)\n",
    "            else:\n",
    "                numerical_cols.append(col)\n",
    "              \n",
    "        # Handle missing values in numerical columns\n",
    "        for col in numerical_cols:\n",
    "            if df[col].isna().any():\n",
    "                median_val = df[col].median()\n",
    "                df.loc[:, col] = df[col].fillna(median_val)\n",
    "        \n",
    "        # Normalize categorical labels with numerical values\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            df.loc[:, col] = le.fit_transform(df[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        feature_cols = numerical_cols + categorical_cols\n",
    "        X = df[feature_cols].values\n",
    "        \n",
    "        # Scale features based on mean and standard deviation\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        self.scalers['main'] = scaler\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        print(f\"Final feature dimension: {X_scaled.shape[1]}\\n\")\n",
    "        \n",
    "        return X_scaled, df['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05f1f2",
   "metadata": {},
   "source": [
    "Deep Autoencoder for anomaly detection in network traffic. Personalized Neural network\n",
    "\n",
    "Architecture\n",
    " \n",
    "    - Encoder: Progressively compresses input to low-dimensional latent space  \n",
    "    - Decoder: Reconstructs original input from latent representation  \n",
    "    - Dropout: Regularization to prevent overfitting  \n",
    "    - BatchNorm: Stabilizes training  \n",
    "    \n",
    "    Anomaly detection principle:\n",
    "    Normal traffic is well-reconstructed (low error)\n",
    "    Anomalous traffic has high reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05db969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Autoencoder for anomaly detection in network traffic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder \n",
    "        # compress the original input into a low-dimensional representation\n",
    "        self.encoder = nn.Sequential(\n",
    "            #Transform the input into a larger space\n",
    "            nn.Linear(input_dim, 256),\n",
    "            #Batch normalization\n",
    "            nn.BatchNorm1d(256),\n",
    "            #Non linear activation function \n",
    "            nn.ReLU(),\n",
    "            #Regularization technique. Reduce overfitting\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            #Compression\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            #Compression\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            #Bottleneck\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder \n",
    "        # try to expand the compressed latent_dim to reconstruct the original input\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            #first expaction\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "    \n",
    "\n",
    "    #needed for training and loss calculation\n",
    "    def forward(self, x):\n",
    "        #compress input\n",
    "        latent = self.encoder(x)\n",
    "        #reconstruct input\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f3f05",
   "metadata": {},
   "source": [
    "Trainning pipeline\n",
    "Operational logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1b1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"Complete pipeline for network anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim=16, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = DeepAutoencoder(input_dim, latent_dim).to(device)\n",
    "        #Mean Squared Error (standard for autorncoder)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        #Optimizer. Weight decay prevent overfitting. lr=learning rate\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        #reduces learning rate of 0.5 if after 3 epochs the loss doesn't improve\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=3, factor=0.5\n",
    "        )\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data,) in enumerate(train_loader):\n",
    "            data = data.to(self.device)\n",
    "            \n",
    "            #Reset previously calculated gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            reconstructed = self.model(data)\n",
    "            loss = self.criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            #Weight Update\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "        \n",
    "    #validate the performance after each epoch\n",
    "    def validate(self, val_loader):\n",
    "        #eval mode\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        #no weight update in validation\n",
    "        with torch.no_grad():\n",
    "            for data, in val_loader:\n",
    "                data = data.to(self.device)\n",
    "                reconstructed = self.model(data)\n",
    "                loss = self.criterion(reconstructed, data)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(val_loader)\n",
    "    \n",
    "    #data preparations, train the model\n",
    "    def fit(self, X_train, X_val, epochs=30, batch_size=256):\n",
    "        print(\"Starting training\\n\")\n",
    "        \n",
    "        # Conversion in tensor\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(X_train))\n",
    "        val_dataset = TensorDataset(torch.FloatTensor(X_val))\n",
    "        \n",
    "        #divide training and validation set in batch\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # Early stopping if loss does not reduce\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 5:\n",
    "                    print(f\"\\n Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        print(\"\\n Training completed!\\n\")\n",
    "    \n",
    "    #final output of the trained model\n",
    "    def predict_reconstruction_error(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.model(X_tensor)\n",
    "            #Calculate the average of the errors along the feature dimension\n",
    "            errors = ((reconstructed - X_tensor) ** 2).mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        return errors\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6da55",
   "metadata": {},
   "source": [
    "Evaluation: \n",
    "\n",
    "Take the reconstruction errors calculated by the Autoencoder and transform them into clear performance metrics to evaluate the effectiveness of the model in detecting anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18140a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(detector, X_normal, X_attack, y_true):\n",
    "    print(\"Evaluating model performance\\n\")\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    errors_normal = detector.predict_reconstruction_error(X_normal)\n",
    "    errors_attack = detector.predict_reconstruction_error(X_attack)\n",
    "    \n",
    "    # Combine for evaluation\n",
    "    all_errors = np.concatenate([errors_normal, errors_attack])\n",
    "    \n",
    "    # ROC-AUC Score (Area Under the Receiver Operating Characteristic Curve)\n",
    "    # Find a threshold value that separates normal traffic from attacks\n",
    "    auc = roc_auc_score(y_true, all_errors)\n",
    "    \n",
    "    # Find optimal threshold using Youden's J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, all_errors)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    # Predictions with optimal threshold. \n",
    "    # Used to convert continuous errors into binary predictions\n",
    "    predictions = (all_errors > optimal_threshold).astype(int)\n",
    "    \n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"\\n ROC-AUC Score: {auc:.4f}\")\n",
    "    print(f\"\\n Optimal Threshold: {optimal_threshold:.6f}\")\n",
    "    print(f\"\\n Reconstruction Error Statistics:\")\n",
    "    print(f\"   Normal Traffic:  {errors_normal.mean():.6f} ± {errors_normal.std():.6f}\")\n",
    "    print(f\"   Attack Traffic:  {errors_attack.mean():.6f} ± {errors_attack.std():.6f}\")\n",
    "    print(f\"   Separation:      {(errors_attack.mean() / errors_normal.mean()):.2f}x\")\n",
    "    \n",
    "    print(f\"\\n Classification Report:\")\n",
    "    print(classification_report(y_true, predictions, \n",
    "                                target_names=['Normal', 'Attack'],\n",
    "                                digits=4))\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'threshold': optimal_threshold,\n",
    "        'errors_normal': errors_normal,\n",
    "        'errors_attack': errors_attack,\n",
    "        'predictions': predictions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529850bd",
   "metadata": {},
   "source": [
    "Main execution pipeline \n",
    "\n",
    "Parameters:\n",
    "    \n",
    "    use_subset : bool\n",
    "        If True, uses a subset of data for faster training\n",
    "\n",
    "    subset_size : int\n",
    "        Number of samples to use if use_subset=True\n",
    "        \n",
    "    contamination_rate : float\n",
    "        Percentage of attacks in training set (0.0 = pure unsupervised, 0.05 = 5% attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(use_subset=True, subset_size=200000, contamination_rate=0.05):\n",
    "    \n",
    "    # Load data\n",
    "    df = load_unsw_nb15()\n",
    "    \n",
    "    # Subset for faster training/testing if dataset too big\n",
    "    if use_subset and len(df) > subset_size:\n",
    "        print(f\"Using subset of {subset_size:,} samples for faster training\")\n",
    "        print(f\" (Full dataset: {len(df):,} samples)\")\n",
    "        \n",
    "        # Stratified sampling to maintain class balance\n",
    "        if 'binary_label' in df.columns:\n",
    "            df, _ = train_test_split(\n",
    "                df, \n",
    "                train_size=subset_size, \n",
    "                stratify=df['binary_label'],\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            df = df.sample(n=subset_size, random_state=42)\n",
    "        print(f\"   Subset created: {len(df):,} samples\\n\")\n",
    "    \n",
    "    # Preprocess\n",
    "    preprocessor = NetworkDataPreprocessor()\n",
    "    X, y = preprocessor.fit_transform(df)\n",
    "    \n",
    "    print(f\"Dataset statistics:\")\n",
    "    print(f\" Total samples: {len(y):,}\")\n",
    "    print(f\" Normal traffic: {sum(y==0):,} ({sum(y==0)/len(y)*100:.1f}%)\")\n",
    "    print(f\" Attack traffic: {sum(y==1):,} ({sum(y==1)/len(y)*100:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Split data with configurable contamination\n",
    "    X_normal = X[y == 0]\n",
    "    X_attack = X[y == 1]\n",
    "    \n",
    "    # Standard anomaly detection: train only on normal\n",
    "    X_train_normal, X_val_normal = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # For evaluation: create balanced test set\n",
    "    X_test_normal, X_test_attack = train_test_split(X_val_normal, test_size=0.5, random_state=42)\n",
    "    X_attack_test, X_attack_remaining = train_test_split(X_attack, test_size=0.6, random_state=42)\n",
    "    \n",
    "    # Training set: mostly normal (can add small contamination for realism)\n",
    "    n_contamination = int(len(X_train_normal) * contamination_rate)\n",
    "    \n",
    "    if n_contamination > 0 and len(X_attack_remaining) > n_contamination:\n",
    "        X_contamination = X_attack_remaining[:n_contamination]\n",
    "        X_train = np.vstack([X_train_normal, X_contamination])\n",
    "    \n",
    "    # Validation set: only normal\n",
    "    X_val = X_test_normal\n",
    "        \n",
    "    # Initialize and train model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\" Device: {device}\")\n",
    "    if device == 'cuda':\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\\n\")\n",
    "    \n",
    "    detector = AnomalyDetector(input_dim=X.shape[1], latent_dim=16, device=device)\n",
    "    \n",
    "    # Adjust epochs based on dataset size\n",
    "    if use_subset and subset_size <= 100000:\n",
    "        epochs = 30\n",
    "    elif use_subset and subset_size <= 500000:\n",
    "        epochs = 20\n",
    "    else:\n",
    "        epochs = 15\n",
    "        \n",
    "    detector.fit(X_train, X_val, epochs=epochs, batch_size=256)\n",
    "    \n",
    "    # Evaluate on balanced test set\n",
    "    X_test = np.vstack([X_test_normal, X_attack_test])\n",
    "    y_test = np.concatenate([np.zeros(len(X_test_normal)), np.ones(len(X_attack_test))])\n",
    "    \n",
    "    # Split test data for evaluation function\n",
    "    results = evaluate_model(detector, X_test_normal, X_attack_test, y_test)\n",
    "        \n",
    "    # Save configuration info\n",
    "    results['config'] = {\n",
    "        'subset_size': subset_size if use_subset else len(df),\n",
    "        'contamination_rate': contamination_rate,\n",
    "        'epochs': epochs,\n",
    "        'device': device,\n",
    "        'features': X.shape[1]\n",
    "    }\n",
    "    \n",
    "    return detector, results, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58531b99",
   "metadata": {},
   "source": [
    "Examples of different main parameters \n",
    "Quick test (fast, ~2-3 minutes):\n",
    "    detector, results, preprocessor = main(use_subset=True, subset_size=50000, contamination_rate=0.05)\n",
    "\n",
    "Medium test (balanced, ~5-8 minutes):\n",
    "    detector, results, preprocessor = main(use_subset=True, subset_size=200000, contamination_rate=0.05)\n",
    "\n",
    "Full dataset (best results, ~20-30 minutes):\n",
    "    detector, results, preprocessor = main(use_subset=False, contamination_rate=0.05)\n",
    "\n",
    "Pure unsupervised (no contamination):\n",
    "    detector, results, preprocessor = main(use_subset=True, subset_size=200000, contamination_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    detector, results, preprocessor = main(\n",
    "        use_subset=True,           # Use subset \n",
    "        subset_size=500000,         \n",
    "        contamination_rate=0.05    # 5% attack contamination\n",
    "    )\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
